{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "跳到主要内容跳到部分\n",
    "SpringerLink数据库\n",
    "搜索 \n",
    "菜单\n",
    "家\n",
    "登录\n",
    "材料建模手册 第1-20页| 引用为\n",
    "\n",
    "材料建模中的神经网络潜力\n",
    "作者\n",
    "作者和附属机构\n",
    "马蒂·赫尔斯特伦电邮作者JörgBehler电邮作者\n",
    "1. \n",
    "生活参考工作条目\n",
    "首先在线： 2018年6月18日\n",
    "抽象\n",
    "可靠的原子间势能的可用性对于进行复杂材料的计算机模拟是必要的。虽然密度泛函理论等电子结构方法已经在许多系统中得到了很大的应用，但这些方法的高计算成本严重限制了可以研究的科学问题。因此，近年来，已经花费了大量精力来开发能够进行大规模模拟的更有效的潜力。特别是，机器学习潜力受到了相当大的关注，因为它们承诺将第一原理方法的准确性与力场效率相结合。在本章中，将回顾和讨论使用人工神经网络的一类重要的机器学习潜力。\n",
    "\n",
    "下载 参考工作条目PDF\n",
    "1简介\n",
    "人工神经网络（NN）非常通用，目前广泛用于机器学习（ML）应用（Bishop 1996 ; Haykin 2011）：语音和手写识别，自动驾驶车辆，商业智能，工业过程控制和游戏，仅举几例。神经网络能够处理ML算法所针对的两种主要类型的问题：分类和回归。本章重点介绍如何将NN用于特定的回归问题，即潜在能量表面（PES）的预测，即作为系统中原子位置函数的势能。这种神经网络被称为神经网络潜在（NNP）（Handley和Popelier 2010; Behler 2011b，2017）。\n",
    "\n",
    "PES是材料建模，理论凝聚态物理和计算化学中许多问题的核心。它给出了不同原子构型的相对稳定性，并且与例如机械性质，缺陷分布，反应速率，热力学平衡，光谱特征和许多其他性质直接相关。\n",
    "\n",
    "有多种计算PES的方法，材料建模领域最常用的方法包括电子结构方法，最突出的密度泛函理论（DFT）（Parr和Yang 1989）），以及各种原子甚至粗粒度的电位。事实证明，DFT计算对许多实验性质具有相当好的预测能力。然而，如果对大型系统进行建模，则DFT计算在计算上变得非常苛刻。这通常将DFT的适用性限制为几百个原子的静态（单点）计算或几百皮秒的短动态模拟。另一方面，原子电势更接近，因此以比DFT低得多的计算成本提供PES的估计。因此，它们实际上可以应用于大规模模拟。所需的参数通常适合于再现系统的一些实验性质，或者再现一些关键结果，如来自DFT计算的能量。后一种方法是所谓的多尺度建模的一个例子，其中来自高水平理论（DFT）的信息用于参数化较低水平的理论。\n",
    "\n",
    "神经网络潜力在很多方面类似于力场。然而，与力场不同，NNP的功能形式不基于任何物理近似。相反，利用了NN的极大灵活性，并且NNP被参数化以再现从DFT参考计算或从一些其他电子结构方法获得的PES。然后，NNP提供了一种计算上廉价的预测PES的方法，这使得在大规模蒙特卡罗和分子动力学模拟中使用NNP来对结构和相空间进行采样成为可能。在NNP参数化期间，NN“学习”不同分子和结构基序的稳定性。因此，NNP本身就是“反应性的，“意味着只要提供相关的培训数据，就可以准确地描述化学反应或显着的结构重排，包括共价键的断裂和形成。对于最终用户，使用神经网络电位或反作用力场之间没有明显差异。两种类型的方法都以相似的计算成本提供PES，并且可以应用于类似类型的反应系统。\n",
    "\n",
    "从Doren及其同事的开创性工作开始，NN已被用于表示潜在能量表面二十多年（Blank等人1995）。第一代NNP受限于仅包含少量原子的小分子系统，但这种限制可以通过引入高维NNP在2007年克服（Behler和Parrinello 2007）。近年来，除神经网络之外的ML方法也以类似的方式用于构建复杂系统的原子模拟的电位。例如，高斯逼近势（Bartók等人，2010），核岭回归等方法（Rupp等人，2012））和支持向量机（Balabin和Lomakina 2011）也可用于描述PES和相关数量。Behler（2016）给出了基于机器学习的原子间势能的最新进展的综述。\n",
    "\n",
    "人工神经网络的整个理论的介绍可以在许多教科书中找到（例如，Bishop 1996 ; Haykin 2011），超出了本章的范围。因此，我们将重点关注通常用于估算给定材料的PES的设计选择。势能是一个实数值，表示原子结构的输入特征是实数值，所以NNP是一个函数。 χ:Rn→R\n",
    "\n",
    "我们将讨论如何使用NNP来计算势能，如何从一组原子位置转换为合适的NN输入，如何参数化NNP，以及如何验证NNP，以及一些NNP的优点和缺点。\n",
    "\n",
    "2前馈神经网络\n",
    "图1显示了一个小的完全连接的前馈NN的示意图，它定义了一个函数，它转换输入向量G  =（G 1，G ^ 2）Ť到输出值ê。该函数具有包含在权重矩阵a （0），b （0），a （1），b （1），a （2）和b （2）中的若干参数，定义E  =  χ χ ：R2→ R （y_1 ^ {（0）}，y_2 ^ {（0）}） （G ; a （0），b （0），a （1），b （1），a （2），b （2））。示例NN包括具有两个节点的输入层，两个每个包含三个节点的隐藏层，以及包含一个节点的输出层。此外，固定值为1的偏置节点连接到隐藏层和输出层中的所有节点（以蓝色显示）。示例NN具有2-3-3-1的架构。层0中的输入（G 1，G 2）对应于值。 （ÿ（0 ）1，y（0 ）2）\n",
    "在新窗口中打开图像图。1\n",
    "图。1\n",
    "2-3-3-1前馈神经网络的图示以及激活函数f （k）（x）的典型选择。该NN定义函数E  =  χ（G 1，G 2 ; a （0），b （0），a （1），b （1），a （2），b （2））。在和之间带有黑色箭头表示元素ÿ（k ）一世 X（k + 1 ）Ĵ 一个（k ）我j在权重矩阵a （k）中 ; 这是针对a （0）的一些元素明确显示的。类似地，蓝色箭头表示偏差权重。带有填充头的箭头代表激活函数f （k）的应用\n",
    "\n",
    "计算从左到右进行：从输入层值，计算第一隐藏层值y （1）  =（y 1 （1），y 2 （1），y 3 （1））T和偏置节点被添加; 然后将这些值用于计算第二隐藏层中的y （2）  =（y 1 （2），y 2 （2），y 3 （2））T并添加偏置节点; 这些值被用于计算最终的输出值ê。\n",
    "\n",
    "前一层中的值y （k -1）用于通过中间向量x （k）计算下一层中y （k）的值：\n",
    "x(k)=(a(k−1))Ty(k−1)+(b(k−1))T.\n",
    "（1）\n",
    "在Eq。在图1中，矢量x （k）的每个元素被计算为（b （k -1））T中的偏置权重和y （k -1）的元素的线性组合之和，其中系数给出权重矩阵a （k -1）。例如，在图1中，x 1 （1）  =  a 11 （0）y 1 （0）  +  a 21 （0）y 2 （0）  + b 11 （0）。a （k -1）和b （k -1）的元素是在NN可用于能量预测之前必须确定的参数（NN权重）; 如何做出这个决定将在后面说明。偏置节点的目的（具有1的值，在图以蓝色显示。1在层）ķ  - 1是添加不依赖于值的一些所需的常量ý （ķ -1） ，对中的每个元素x （k）（即到下一层）。向量x （k）的值然后通过激活函数（有时称为传递函数或基函数）f （k）（x）进行变换：\n",
    "ÿ一世（k ）= f（k ）（x一世（k ））\n",
    "（2）\n",
    "对于隐藏层，通常使用非线性S形激活函数，例如，逻辑函数或双曲正切f（x）= tanh（x）。隐藏层激活功能还有其他可能的选择。对于输出层，线性激活函数f（x）=  x用于避免NN的输出值的受约束范围。 F（x ）= 11 + e x p（- x ）\n",
    "\n",
    "对于NN来预测系统的势能，输入矢量G必须包含有关确定潜在能量的系统的所有信息。在没有任何外部场的情况下，势能在旋转和平移下是不变的。因此，糟糕的选择是简单地选择原子的笛卡尔坐标作为输入向量，因为它们不具有这些不变性。更好的选择是内部坐标，如原子间距离和键角，设计在旋转和平移下是不变的。只要内部坐标的数量是可控的，例如，描述小分子的PES，这就可以很好地工作。然而，对于包含数千或更多自由度的大型系统，由于输入特征的数量变得非常大，因此该方法很快变得难以处理。\n",
    "\n",
    "不是让NN描述整个系统的PES（“低维”NNP），而是可以构造描述单个原子的PES的NN。通过将几个这样的原子NN组合在一起，可以限制输入特征的数量，同时保留对具有数千个原子的大系统进行建模的能力。这些高维NNP将成为本章其余部分的重点。高维NNP通常与所谓的对称函数一起用作输入特征。它们将在下一节中介绍。\n",
    "\n",
    "3高维神经网络电位和对称函数\n",
    "在高维神经网络潜力中，总能量被计算为“原子”能量E i的总和：\n",
    "E=∑i=1NatomEi\n",
    "（3）\n",
    "其中N atom是系统中的原子数。每个原子能量E i通过依赖于元素的前馈NN来确定，其中输入特征描述原子周围的化学环境。这类似于如何使用原子环境的总和（例如，键长和角度）来计算许多力场中的总能量。然而，高维NN具有相当灵活的功能形式，并且可以容易地捕获高阶多体效应; 实际上，输入特征通常使用称为对称函数的特定多体函数来计算。\n",
    "对于具有多个元素的多组分系统，每个元素使用一个NN。例如，为了描述CuAgAu合金，产生三个元素NN，一个用于Cu，一个用于Ag，一个用于Au，并且每个NN对于相应元素的每个原子被调用一次。不同的NN架构（输入特征的数量，隐藏层的数量和/或每个隐藏层的节点的数量）可以用于不同的元素。系统中所有Cu原子的原子能将使用相同的Cu特异性NN来评估，只有输入矢量的数值不同，因为不同原子周围的原子环境可能不同。该处理确保总能量相对于输入文件中提供Cu原子的顺序是排列不变的。图2图1示意性地示出了如何使用高维神经网络电势计算总能量E.\n",
    "在新窗口中打开图像图2\n",
    "图2\n",
    "使用高维神经网络电位评估具有100个原子和两个元素I和J的系统的总能量E的示意图。在该示例中，N sym（I）= 40并且N sym（J）= 30.对每个原子能量E i的评估如图1中那样进行。\n",
    "\n",
    "下面，我们指的是使用的索引特定原子我，Ĵ，和ķ以及使用大写字母的元素我，Ĵ，和ķ，并且我们使用符号我  ∈  予以指定原子我是元件的予。所有原子位置的集合与对应的元素一起表示为{ R，Z }，其中{ R }表示笛卡尔坐标，Z表示元素。因此，{ R，Z}包含通常包含在蒙特卡罗模拟的单个帧或分子动力学模拟的单个帧中的所有信息（没有粒子速度）。\n",
    "\n",
    "对于一个原子我  ∈  我，原子能ê 我等式 计算为3\n",
    "Ë我∈ 我= χ一世（G.一世（i ））\n",
    "（4）\n",
    "其中χ 我表示NN为元件我。输入向量G I（i）是对称函数值的向量：\n",
    "G一世（我∈ 我）= ⎛⎝⎜⎜⎜⎜⎜⎜⎜G一世1（i ，{ R，Z} ）G一世2（i ，{ R，Z} ）⋮G一世ñ小号ÿ 米（我）（i ，{ R，Z} ）⎞⎠⎟⎟⎟⎟⎟⎟⎟\n",
    "（5）\n",
    "对称函数是原子周围化学环境的描述符。通常，仅考虑半径为R cut的截止球内的局部环境。这是通过锥形函数f cut（R）实现的，该函数f cut（R）平滑地衰减到0并且在R  =  R cut时斜率。f cut（R）的常见选择是\n",
    "FÇ ü Ť（R ）= { tanh3（ 1 - R.[RÇ ü Ť）0[R ≤ [RÇ ü ŤR > R.Ç ü Ť\n",
    "（6）\n",
    "尽管其他形式的˚F 切割也是可能的。用于原子“径向对称”的功能的一个例子我  ∈  我是Behler（2011A）\n",
    "G一世（我∈ 我，{ R，Z} ;η，Rs h i fŤ，J）= φ ⎛⎝⎜⎜ΣĴ ∈ Ĵj ≠ iË- η（R我j- R.s h i fŤ）2⋅ ˚FÇ ü Ť（R我j）⎞⎠⎟⎟。\n",
    "（7）\n",
    "此函数是如何元素的原子的描述符Ĵ（即可以是相同或不同的我）周围的原子分布我  ∈  予。它是高斯的总和乘以截止函数f cut（R）。可选特征缩放功能φ修改由对称函数G I输出的值的范围，这在NN参数化期间可能是有用的。缩放功能将在Sect中进一步讨论。4.2。两个参数η和R shift确定高斯函数的宽度和中心。数字图3a示出了等式中的加数的值。对于不同的距离，图 7示出了几个选定的 η和 R shift值。这里，截止距离设置为 R cut  =6Å的典型值，并且截止函数来自Eq。使用6。黑线（ η  =0Å -2）相当于普通截止函数 f cut（ R）。\n",
    "在新窗口中打开图像图3\n",
    "图3\n",
    "（a）方程中径向对称函数的加数。在图7中，对于η和R shift的一些不同值，使用来自等式1 的截止函数f cut（R）。6。（b）方程式中对称函数的加数的角部分。在图8中，对于一些选定的ζ和λ值\n",
    "\n",
    "通常，对于中心元素I和相邻元素J的每个组合，在等式1中的类型的若干函数。具有不同的η和/或R shift的值的图7用作等式1中的NN的输入特征。5。使用几个这样的对称函数提供了比仅使用单个对称函数可以实现的明显更好的原子环境指纹。例如，如果使用图3中 η  =0Å -2的单个函数，则原子i周围的两个邻居（距离R  = 3）产生相同的值。G I（i）= 0.197作为距离R  = 2 的单个邻居。通过使用几个对称函数，目的是将关于原子i周围的原子环境的所有相关信息“编码” 到输入向量G I（i）中，并提供原子环境的结构指纹作为NN的输入。 。\n",
    "\n",
    "方程中的径向对称函数。图7仅与距离有关，或者换句话说是球对称的。因此，还需要将角度依赖性结合到对称函数中。角度对称函数的常见选择是Behler（2011a）\n",
    "G一世（我∈ 我，{ R，Z} ;η，ζ，λ ，J，K）  = φ ⎛⎝⎜⎜⎜⎜⎜21 - ζΣĴ ∈ Ĵ，ķ ∈ ķj ≠ i ，k ≠ ik ≠ j（1个+ λ COSθĴ 我ķ）ζ⋅ è- η（R2我j+ R.2我ķ+ R.2j k）⋅ ˚FÇ ü Ť（R我j）⋅ ˚FÇ ü Ť（R我ķ）⋅ ˚FÇ ü Ť（Rj k）⎞⎠⎟⎟⎟⎟⎟\n",
    "其中原子间距离- [R IJ，- [R IK，和- [R JK和角度θ JIK三个原子之间我  ∈  我，Ĵ  ∈  Ĵ，和ķ  ∈  ķ被用于计算对称函数的值，对于每个可能的唯一组合邻居j和k围绕中心原子i。同样，元素J和K可以与I和φ相同或不同是缩放功能。在Eq。8，ζ确定的量，角术语是大约0角的范围，并且λ呈现出的任+ 1或值-示于图1中的对称函数的角度部分3 B，几个不同ζ和λ的值。值得注意的是，角度部分是周期性的，周期为360∘，对称性约为0∘和± 180∘。\n",
    "通过使用截止半径，在截止处截断了诸如静电相互作用的长程相互作用。对于许多类型的系统，这可能是有问题的。使用高维NNP将长程相互作用包括在PES中的方法在Sect。5。\n",
    "\n",
    "高维NNP也可用于评估作用于原子的分析力，这对分子动力学模拟等应用至关重要。相对于某个原子坐标α的力是\n",
    "Fα= - ∂Ë∂α= - Σj = 1ñ一个吨Ô 米∂ËĴ∂α= - ΣĴ∈ { Z}ΣĴ ∈ ĴΣμ = 1ñ小号ÿ 米（J.）∂ËĴ∂GĴμ（j ）＆CenterDot;＆＆PartialD;＆GĴμ（j ）∂α\n",
    "（8）\n",
    "其中最外面的总和运行在所有的化学元素Ĵ在系统中，并且其中是μ个为元素对称函数Ĵ评价了原子Ĵ  ∈  Ĵ。 GĴμ（j ）\n",
    "4构建高维NNP\n",
    "某些给定化学系统的高维NNP的构建是一个涉及的过程\n",
    "采购培训和验证数据\n",
    "\n",
    "每个元素的对称函数（包括截止半径R cut）的选择\n",
    "\n",
    "每个元素的网络架构选择\n",
    "\n",
    "对每个元素拟合权重矩阵a （k）和b （k）\n",
    "\n",
    "此外，必须严格评估拟合的NNP并检查潜在能量表面的未充分描述的区域。\n",
    "在高维NNP的典型应用中，每个元素NN使用1-3个隐藏层和每个隐藏层10-40个节点。对称函数的数量很大程度上取决于系统中化学元素的数量。通常，对于元件I，使用每个可能的相邻元件J的 5-10个径向对称函数（公式7），以及每个相邻元件J和K的可能组合的5-10个角对称函数（公式8）。对于所有对称函数，截止距离R cut通常设定在6到10的范围内。\n",
    "\n",
    "4.1培训数据的采购\n",
    "采购训练数据对于成功应用高维神经网络潜力至关重要。神经网络只能与训练过的数据一样好。因此，选择对建模材料准确的参考计算方法是很重要的。通常，某种形式的密度泛函理论（DFT）被用作参考方法。然后训练集由一组结构组成，其能量已经使用DFT确定。可选地，作用于原子的DFT力（其包含关于PES的有价值的局部信息）也可用于训练NN。训练程序的目的是让NN尽可能地重现参考能量和力量; 这是通过迭代地优化权重矩阵来实现的每个元素NN的（k）和b （k）。\n",
    "\n",
    "在典型设置中，越来越多的训练数据被迭代地添加到训练集中，以便获得更好的神经网络电位。从训练数据的一些初始集合，数个高维神经网络电位χ 0 （0） ，χ 0 （1） ，等被训练（如在下一节中所描述的）。通过将这些电位应用于例如分子动力学模拟，可以识别描述不佳的结构。这可以通过以下几种方式实现：\n",
    "1。\n",
    "通过监视模拟中出现的不同结构的对称函数的值。如果对称函数值位于训练集中出现的值范围之外，或者如果对称函数的值仅在训练集中很少出现，则由神经网络进行的预测可能是不准确的。\n",
    "\n",
    " \n",
    "2。\n",
    "通过比较能量和由不同的神经网络预测的力χ 0 （0） ，χ 0 （1）等上的相同结构。如果神经网络的不同配合的能量和/或力非常不同，则配置空间的相应部分未被充分采样。\n",
    "\n",
    " \n",
    "在上述任何方法识别的结构然后重新计算使用的参考方法和添加到该训练集，随后是新的NN优化，从而产生新的NN电位χ 1 （0） ，χ 1 （1） ，等等。在该方式，越来越多的训练数据被迭代地添加到训练集，直到NN被认为准确地描述了模拟中出现的所有结构的PES。\n",
    "4.2对称函数的选择\n",
    "对称函数的矢量，公式中的G I. 如图5所示，对于每个元素I，允许NNP区分不同的结构。因此，在为某些化学系统设计NNP时，一个重要的考虑因素是在评估G I时使用的对称函数。如果仅使用少数对称函数，则NNP可能无法可靠地区分不同的结构。 G一世μ\n",
    "\n",
    "在实践中，通常根据经验选择和改进对称函数集，直到NNP给出令人满意的结果。“良好”的对称函数集通常包含满足以下条件的函数：\n",
    "对于训练集中的所有原子，给定对称函数的值不相同。更一般地，当针对训练集中的不同原子计算时，某个对称函数的值的范围不应该太小，因为NN然后可以将总能量的大的变化分配给对称函数值的小变化。另外，应该分析某个对称函数的值的分布，以确保值的范围不受少数异常值的支配。\n",
    "\n",
    "这组对称函数涵盖了一系列化学上有意义的距离。例如，最快衰减的径向对称函数（具有最大值η，等式7的函数）应该围绕元素I和J的原子之间的最短可能有意义的键的距离衰减。\n",
    "\n",
    "经历非常不同的力（由参考方法计算）的原子必然存在于基本上不同的环境中，因此对于至少一个对称函数应该具有基本上不同的值。如果不是这种情况，则需要增加对称函数集。\n",
    "\n",
    "任何两个对称函数之间的相关性不应太大。\n",
    "\n",
    "将输入特征“预处理”到神经网络可能是有用的。方程式中的示例对称函数中的可选缩放函数φ。例如，图7和图8可以确保训练集中的不同结构的对称函数值都位于某个预定范围内，例如[-1,1]。的定义φ μ会话，对于对称函数G ^ μ，是\n",
    "φμ（x ）= 2 （x - G.∘ ，米我Ñμ）G∘ ，m a xμ- G.∘ ，米我Ñμ- 1\n",
    "（9）\n",
    "其中和是获得的训练集中的最小值和最大值相应的未缩放对称函数。这种特征缩放有助于平衡不同对称函数的相对重要性。还可以使用其他类型的特征缩放功能。 G∘ ，米我Ñμ G∘ ，m a xμ G∘μ\n",
    "4.3 NN权重的优化\n",
    "每个化学元素的NN权重参数a （k）和b （k）的优化从它们的值的初始猜测开始。初始权重可以只是随机数，但也有几种方法可以选择初始权重，以便最小化训练神经网络所需的时间，例如，Nguyen和Widrow开发的方案（1990年））。设置连接最后隐藏层和输出层的权重也是有用的，这样初始预测能量的平均值和标准偏差（在任何训练之前）与训练集中参考能量的平均值和标准偏差相匹配。参考能量总是提供一些单位; 权重可以被认为是无量纲数，能够以正确的单位提供NN能量。\n",
    "\n",
    "通过迭代地最小化成本函数Γ来实现权重的优化。优化迭代通常被称为时期。成本函数经常被认为是NN计算值和参考值之间的平方差的加权平均值。例如，成本函数的能量，Γ ē，是\n",
    "ΓË= 1ñ小号吨- [R ù Ç 吨Σi = 1ñ小号吨- [R ù Ç 吨w ^我，E（E.N N.一世- E.R e f一世ñ一个吨Ô 米一世）2\n",
    "（10）\n",
    "其中N struct是训练集中结构的数量，w i，E是依赖于结构的权重参数，而是第i个结构中的原子数。通过为训练集中的不同结构选择不同的w i，E值，可以为训练集中的某些结构赋予比对其他结构更大的重要性。注意，表示结构i在训练集中的重要性的权重w i，E由用户设置并且不适合，与NN权重不同 ñ一个吨Ô 米一世a （k）和b （k）。在实践中，通常将相同的权重w i，E  = 1 分配给训练集中的所有结构。\n",
    "同样，成本函数的力量，Γ ˚F，成为\n",
    "ΓF= 1ñ小号吨- [R ù Ç 吨Σi = 1ñ小号吨- [R ù Ç 吨⎡⎣w ^我，F3 N.一个吨Ô 米一世Σj = 13 N.一个吨Ô 米一世（F.N N.Ĵ- F.R e fĴ）2⎤⎦\n",
    "（11）\n",
    "其中内部总和取自训练集结构i中的所有力分量。如果NN优化为在能量和力两者，所以能够在之间的任一方交替Γ Ë和Γ ˚F成本函数对于每个优化步骤（其中，NN的权重一（ķ）和b （ķ）被更新），或将它们组合成一个单一的成本函数 3 N.一个吨Ô 米一世\n",
    "Γ= ΓË+ ΓF\n",
    "（12）\n",
    "在这种情况下，依赖于结构的权重w i，F通常被设置为比w i，E小得多的数字，因为对于结构i，只有单个能量E i而是的力分量。成本函数的表达式还可以取决于优化算法; 例如，一些算法包括成本函数中的历史相关项。 3 N.一个吨Ô 米一世\n",
    "存在许多可能的优化算法。最简单的一个，最陡的下降（或“反向传播”（Rumelhart等人，1986）），根据以下规则更新权重：\n",
    "一个我j（k ）（t + 1 ）= a我j（k ）（t ）- η＆CenterDot;＆＆PartialD;＆Γ∂一个我j（k ）（t ）\n",
    "（13）\n",
    "其中t是纪元号，η是学习率。对每个元素NN执行该步骤。然而，其他权重优化算法也存在并且经常使用，例如Levenberg-Marquardt算法（Levenberg 1944 ; Marquardt 1963）和全局扩展卡尔曼滤波器（Haykin 2001）。这些算法的描述不在本章的范围之内。\n",
    "通常，一组NN权重的质量由能量和力的均方根误差RMSE表征：\n",
    "R M S E（E）p o r a t o m = 1ñ小号吨- [R ù Ç 吨Σi = 1ñ小号吨- [R ù Ç 吨（E.N N.一世- E.R e f一世ñ一个吨Ô 米一世）2- - - - - - - - - - - - - - - - - - - - - - - - ⎷\n",
    "（14）\n",
    "R M S E（F）= 1ñ小号吨- [R ù Ç 吨Σi = 1ñ小号吨- [R ù Ç 吨⎡⎣13 N.一个吨Ô 米一世Σj = 13 N.一个吨Ô 米一世（F.N N.Ĵ- F.R e fĴ）2⎤⎦- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ⎷\n",
    "（15）\n",
    "RMSE（E）通常被报告为“每个原子”归一化的值，因为与含有少量原子的结构相比，含有许多原子的结构通常具有更大的能量绝对误差。对于RMSE（F），内部和取自训练集中第i个结构的所有力分量。 3 N.一个吨Ô 米一世\n",
    "\n",
    "一个好的NN将具有RMSE的小值。文献中报道的典型值是每个原子的RMSE（E）= 1meV和RMSE（F）= 100 meV /Å。然而，什么构成良好的拟合取决于NN将如何应用。此外，RMSE仅提供了训练的NN对训练数据的执行情况的简单测量。如果训练数据变化很大，例如，就系统的化学成分而言，训练集中所有结构的RMSE平均值并不一定有助于科学家发现结构或化学成分不能很好地描述。 NN。在这种情况下，最好将训练集分成几个不同的组，例如，根据组成分别计算每组的RMSE。此外，探索NN所产生的能量和力的误差分布是有帮助的。如果NN对某些特定结构的表现不佳，w E和w F在训练过程中（方程10和11）或修改对称函数集，以便更好地描述原子化学环境。\n",
    "\n",
    "4.4训练集，验证集，测试集，过度拟合和欠拟合\n",
    "采购的参考数据通常分为训练集和测试集。训练集包括用于确定NN权重参数的结构。测试集包含不用于训练NN的附加参考数据。通过使用测试装置，可以通过监测两组的成本函数来评估所装配的NN的质量。\n",
    "\n",
    "测试集的使用有助于检测所谓的过度拟合（也称为高方差）。过度拟合的一个例子如图4所示a在一个简单的一维情况下：尽管由蓝线表示的NN非常好地再现了每个训练点（红色圆圈）的参考值，但NN对位于训练集之外的值（橙色三角形）进行了非常不准确的预测。因此，虽然训练集上的RMSE很小，但测试集的RMSE会大得多。这表明NN在训练集中的数据点之间进行了差的插值。过度拟合通常是由于神经网络结构太大而发生的，例如，关于隐藏层的数量或每个隐藏层的节点数量。通过在较早的时期停止NN权重优化，可以通过减小神经网络的大小来对抗过度拟合，Γ。正则化项惩罚权重a ij（k）的大值，并且许多类型的正则化项是可能的（例如，所谓的L 1和L 2正则化）。关于与神经网络相关的正则化的更深入讨论可以在例如Haykin（2011）中找到。\n",
    "在新窗口中打开图像图4\n",
    "图4\n",
    "（a）过度拟合，（b）不合适，（c）良好拟合的例子。红色圆圈表示训练集中的数据点; 橙色三角形代表测试集中的点\n",
    "\n",
    "过拟合的相反的欠拟合（也称为具有高偏压），在图中所示4湾 欠拟合的特征在于训练和测试集的高RMSE，并且通常在NN权重优化没有运行足够的时期时发生。如果运行更多的时期不能解决欠载问题，则可以帮助使神经网络功能形式更加灵活，例如，通过向隐藏层添加更多节点。\n",
    "\n",
    "图4 1c示出一个“般配”，其中NN预测和基准数据之间的误差小，以及大致相等两者的训练集和测试集。\n",
    "\n",
    "如果为了解决例如过度拟合的目的而评估若干神经网络架构，则存在最终选择的神经网络架构在测试集上具有欺骗性小错误的危险，因为测试集用于选择神经网络架构。在这种情况下，用于评估NN架构的“测试集”通常被称为验证集，然后针对另一个独立的测试集评估最终选择的NN的误差。\n",
    "\n",
    "5包含远程静电\n",
    "Sect中描述的神经网络潜力。3依赖于对称函数值的向量作为输入，并且对于任何给定的原子，只有环境中的原子（在截止半径R cut的球体内）有助于对称函数值。因此，在Sect。中描述的高维NNP类型的基本假设。图3的实施例是可以从局部原子环境计算系统的势能。某些类型的相互作用，特别是静电相互作用，随着距离的增加而缓慢衰减，并且即使两个相互作用的原子之间的距离大于R 切割，也会对能量和力产生相当大的影响。\n",
    "\n",
    "静电贡献的计算需要有关电荷分布的信息。这可以通过例如将每个原子与点电荷相关联来实现。这些电荷的符号和大小可以固定在某些预定的（元素相关的）值，或者以某种方式“在运行中”确定。与NNP结合使用的后一种方法的一个例子是使用第二种不同的NNP（Artrith等人2011 ; Morawietz等人2012））。然后，第二个NNP将适合再现原子电荷，类似于先前引入的NNP的类型如何用于确定原子能量。通过这种方法，必须在训练集中提供对原子电荷的一些近似。通常，使用可以从电子结构计算得到的许多类型的原子电荷中的一种，例如Mulliken指控（Mulliken 1955），Bader指控（Bader 1985）或Hirshfeld指控（Hirshfeld 1977）。图5说明了这种方法，其中一个高维NNP用于计算“短程”能量E short只依赖于局部原子环境，第二个高维NNP用于估算用于计算“远程”能量E long的原子电荷（通过例如库仑定律或Ewald求和的应用） ）。然后获得势能为E  =  E short  +  E long。用于计算原子电荷的不同元素的NN架构和对称函数不必与用于计算原子能量的那些相同。\n",
    "在新窗口中打开图像图5\n",
    "图5\n",
    "两个高维神经网络电位的插图，使用与图2相同的化学系统和颜色编码。上NN（白色背景）产生短程能量E short ; 较低的NN（灰色背景）产生用于计算E long的原子电荷q i。他们一起给出了总能量E.\n",
    "\n",
    "无论用于产生原子电荷和计算E long的方案如何，至关重要的是短距离NN适合仅再现E tot和E long之间的差异，以避免重复计算对E的贡献。总能量。在E long的计算中，已经证明使用在短距离屏蔽的库仑势是有益的，因为否则NN可能需要为短程能量拟合更波纹的势能面。\n",
    "\n",
    "相对于某个原子坐标α的力变为\n",
    "Fα= F.小号ħ Ò ř 吨α+ F.升Ò Ñ 克α= - ∂Ë小号ħ Ò ř 吨∂α- ∂Ë升Ò Ñ 克∂α\n",
    "（16）\n",
    "其中的计算公式如下。8。对于非周期系统，可以计算为 F小号ħ Ò ř 吨α F升Ò Ñ 克α\n",
    "FlongαqiqjRij==−12∂∂α∑i=1Natom∑j=1j≠iNatom−12∑i=1Natom∑j=1j≠iNatom1R2ij[∂qi∂αqjRij+qi∂qj∂αRij−qiqj∂Rij∂α]\n",
    "（17）\n",
    "其中q i是原子i上的电荷。如果使用具有对称函数的NNP来确定原子电荷，如图5所示，则可以表明\n",
    "F升Ò Ñ 克α= ΣĴ∈ { Z}ΣĴ ∈ ĴΣi = 1我≠ jñ一个吨Ô 米q一世[R我j⋅ [ 12qĴ[R我j∂[R我j∂α- Σμ = 1ñ小号ÿ 米（J.）∂qĴ∂GĴμ（j ）∂GĴμ（j ）∂α]。\n",
    "（18）\n",
    "对于周期系统，其中使用例如Ewald求和来评估静电能量，的表达式变得更复杂，尽管它可以以类似的方式导出。 F升Ò Ñ 克α\n",
    "6神经网络电位的应用和局限\n",
    "专业NNP，并且特别是高维专业NNP，已经开发并应用到一系列不同的分子和材料的（综述参见Behler 2014，2017年）。一些例子包括硅（Behler等人，2008），碳（Khaliullin等人2011），钠（Eshet等人，2012），氧化锌（Artrith等人，2011），碲化锗（Sosso等人，2012），铜（Artrith和Behler 2012），ZnO上的Cu团簇（Artrith等人2013），Cu-Au纳米合金（Artrith和Kolpak 2015），Cu上的水（Natarajan和Behler 2016），二氧化钛（Artrith和Urban）2016），黄金（Boes等人 2016），铜 - 钯 - 银合金（Hajinazar等人 2017），Ru上的N 2（Shakouri等人 2017）和ZnO上的水（Quaranta等人 2017）。\n",
    "\n",
    "由于使用这种灵活的方法可以以低计算成本获得前所未有的准确度，NNP和NNP方法的开发以及其他类似的ML潜在方法的开发目前是非常活跃的研究领域。高维NNP的一些限制和缺点，如本章所述，包括以下内容：\n",
    "大型训练集的迭代构造。典型的训练集包括必须使用参考电子结构方法计算的数千个结构。此外，新训练集结构的生成通常以经验方式进行，例如使用高温分子动力学模拟。\n",
    "\n",
    "化学系统中元素数量有限。系统中有超过三种或四种化学元素，描述原子周围局部化学环境所需的对称函数的数量变得非常大。对于这样的系统，另一种类型的输入特征可能是有益的。\n",
    "\n",
    "复杂且耗时的装配程序。NNP包含许多需要安装的权重。即使使用先进的拟合算法，获得良好的拟合通常也是一个耗时的过程。包括如Sect中所述的远程静电会加剧这个问题。5。\n",
    "\n",
    "7总结\n",
    "近年来，神经网络电位（NNP）已成为原子材料建模模拟的非常有用的工具。它们以低计算成本提供系统的势能面（PES），而精度非常接近第一原理方法。由于NNP的灵活功能形式，它们可用于在平等的基础上描述所有类型的原子相互作用（共价键合，分散相互作用，氢键键合等）。在显着的长程静电相互作用的情况下，NNP可以通过一些用于确定参考原子电荷或多极的方案来扩展，以包括对总能量的长程贡献。虽然NNP适合从电子结构计算中重现参考数据（通常是总能量和原子力），由于本质上非物理但纯粹的数学函数形式，需要仔细验证所获得的电位。在高维NNP的情况下，系统中的每个化学元素都适合一个NN。然后，元素特定的NN描述特定类型原子周围的PES。使用例如对称函数将局部原子环境转换为NN的输入。然后通过对所有原子贡献求和来计算总能量。例如，对称函数。然后通过对所有原子贡献求和来计算总能量。例如，对称函数。然后通过对所有原子贡献求和来计算总能量。\n",
    "\n",
    "与参考电子结构方法相比，与NNP相关的误差可以做得非常小（每个原子<1meV），这使得NNP成为未来在材料建模和模拟中应用的有前途的工具。\n",
    "\n",
    "参考\n",
    "Artrith N，Behler J（2012）金属表面的高维神经网络电位：铜的原型研究。Phys Rev B 85：045439\n",
    "ADS CrossRef Google学术搜索\n",
    "Artrith N，Kolpak AM（2015）使用反应性ANN电位在热平衡中模拟Cu-Au纳米合金的大规模分子动力学模拟。Comput Mater Sci 110：20\n",
    "CrossRef Google学术搜索\n",
    "Artrith N，Urban A（2016）用于原子材料模拟的人工神经网络电位的实现：TiO 2的性能。Comput Mater Sci 114：135-150\n",
    "CrossRef Google学术搜索\n",
    "Artrith N，Morawietz T，Behler J（2011）多组分系统的高维神经网络电位：氧化锌的应用。Phys Rev B 83：153101\n",
    "ADS CrossRef Google学术搜索\n",
    "Artrith N，Hiller B，Behler J（2013）金属和氧化物的神经网络电位 - 首次应用于氧化锌的铜簇。Phys Status Solidi B 250：1191-1203\n",
    "ADS CrossRef Google学术搜索\n",
    "Bader R（1985）分子中的原子。Acc Chem Res 18：9\n",
    "CrossRef Google学术搜索\n",
    "Balabin RM，Lomakina EI（2011）支持向量机回归（LS-SVM） - 用于量子化学数据分析的人工神经网络（ANN）的替代方案？Phys Chem Chem Phys 13：11710\n",
    "CrossRef Google学术搜索\n",
    "BartókAP，Payne MC，Kondor R，CsányiG（2010）高斯近似势：量子力学的准确性，没有电子。Phys Rev Lett 104：136403\n",
    "ADS CrossRef Google学术搜索\n",
    "Behler J（2011a）以原子为中心的对称函数，用于构建高维神经网络势。J Chem Phys 134：074106\n",
    "ADS CrossRef Google学术搜索\n",
    "Behler J（2011b）神经网络势能 - 化学中的能量表面：用于大规模模拟的工具。Phys Chem Chem Phys 13：17930-17955\n",
    "CrossRef Google学术搜索\n",
    "Behler J（2014）通过高维神经网络电位表示势能面。J Phys Condens Matter 26：183001\n",
    "CrossRef Google学术搜索\n",
    "Behler J（2016）观点：原子模拟的机器学习潜力。J Chem Phys 145（17）：170901\n",
    "ADS CrossRef Google学术搜索\n",
    "Behler J（2017）第一原理神经网络潜力用于大分子和浓缩系统的反应模拟。Angew Chem Int Ed 56：12828\n",
    "CrossRef Google学术搜索\n",
    "Behler J，Parrinello M（2007）高维势能面的广义神经网络表示。Phys Rev Lett 98：146401\n",
    "ADS CrossRef Google学术搜索\n",
    "Behler J，MartoňákR，Donadio D，Parrinello M（2008）Metadynamics使用高维神经网络潜力模拟硅的高压相。Phys Rev Lett 100：185501\n",
    "ADS CrossRef Google学术搜索\n",
    "Bishop CM（1996）用于模式识别的神经网络。牛津大学出版社，牛津\n",
    "zbMATH 谷歌学者\n",
    "空白TB，Brown SD，Calhoun AW，Doren DJ（1995）势能面的神经网络模型。J Chem Phys 103：4129-4137\n",
    "ADS CrossRef Google学术搜索\n",
    "Boes JR，Groenenboom MC，Keith JA，Kitchin JR（2016）神经网络和Au属性的ReaxFF比较。Int J Quantum Chem 116：979-987\n",
    "CrossRef Google学术搜索\n",
    "Eshet H，Khaliullin RZ，KühneTD，Behler J，Parrinello M（2012）在高压下钠的异常熔化行为的微观起源。Phys Rev Lett 108：115701\n",
    "ADS CrossRef Google学术搜索\n",
    "Hajinazar S，Shao J，Kolmogorov AN（2017）基于神经网络的多组分材料原子间模型的分层构造。Phys Rev B 95：014114\n",
    "ADS CrossRef Google学术搜索\n",
    "Handley CM，Popelier PLA（2010）通过人工神经网络拟合的势能面。J Phys Chem A 114：3371-3383\n",
    "CrossRef Google学术搜索\n",
    "Haykin S（2001）卡尔曼滤波和神经网络。Wiley，Hoboken\n",
    "CrossRef 谷歌学者\n",
    "Haykin S（2011）神经网络和学习机器。Pearson Education，New Dehli\n",
    "Google学术搜索\n",
    "Hirshfeld FL（1977）用于描述分子电荷密度的键合原子片段。Theor Chim Acta 44：129-138\n",
    "CrossRef Google学术搜索\n",
    "Khaliullin RZ，Eshet H，KühneTD，Behler J，Parrinello M（2011）直接石墨 - 金刚石相变的成核机理。Nat Mater 10：693-697\n",
    "ADS CrossRef Google学术搜索\n",
    "Levenberg K（1944）一种解决最小二乘法中某些非线性问题的方法。Quart Appl Math 2：164-168\n",
    "MathSciNet CrossRef Google学术搜索\n",
    "Marquardt DW（1963）一种非线性参数最小二乘估计算法。SIAM J Appl Math 11：431-441\n",
    "MathSciNet CrossRef Google学术搜索\n",
    "Morawietz T，Sharma V，Behler J（2012）基于环境依赖的原子能和电荷的水二聚体的神经网络势能面。J Chem Phys 136：064103\n",
    "ADS CrossRef Google学术搜索\n",
    "Mulliken RS（1955）LCAO-MO分子波函数的电子总体分析。I. J Chem Phys 23：1833\n",
    "ADS CrossRef Google Scholar\n",
    "Natarajan SK，Behler J（2016）神经网络分子动力学模拟固液界面：低指数铜表面的水。Phys Chem Chem Phys 18：28704\n",
    "CrossRef Google学术搜索\n",
    "Nguyen DH，Widrow B（1990）神经网络用于自学习控制系统。IEEE Con​​trol Syst Mag 3：18-23\n",
    "CrossRef Google学术搜索\n",
    "Parr RG，Yang W（1989）原子和分子的密度泛函理论。牛津大学出版社，牛津\n",
    "谷学者学者\n",
    "Quaranta V，HellströmM，Behler J（2017）水 - ZnO界面的质子转移机制：预溶剂化的作用。J Phys Chem Lett 8：1476\n",
    "CrossRef Google学术搜索\n",
    "Rumelhart DE，Hinton GE，Williams RJ（1986）通过反向传播误差来学习表示。Nature 323：533-536\n",
    "ADS CrossRef Google学术搜索\n",
    "Rupp M，Tkatchenko A，MüllerKR，von Lilienfeld OA（2012）利用机器学习快速准确地建模分子雾化能量。Phys Rev Lett 108：058301\n",
    "ADS CrossRef Google学术搜索\n",
    "Shakouri K，Behler J，Meyer J，Kroes GJ（2017）反应气体 - 表面动力学中表面声子的精确神经网络描述：N 2 + Ru（0001）。J Phys Chem Lett 8：2131\n",
    "CrossRef Google学术搜索\n",
    "Sosso GC，Miceli G，Caravati S，Behler J，Bernasconi M（2012）相变材料GeTe的神经网络原子间势。Phys Rev B 85：174103\n",
    "ADS CrossRef Google学术搜索\n",
    "版权信息\n",
    "©Springer International Publishing AG，Springer Nature 2018的一部分\n",
    "部门编辑和附属机构\n",
    "罗伯托汽车1\n",
    "Biswajit Santra2\n",
    "1. 化学系普林斯顿大学普林斯顿美国\n",
    "2. 普林斯顿大学普林斯顿美国\n",
    "关于此条目\n",
    "CROSSMARK\n",
    "引用此条目为：\n",
    "HellströmM。，Behler J.（2018）材料建模中的神经网络潜力。在：Andreoni W.，Yip S.（编辑）材料建模手册。施普林格，湛\n",
    "首次在线\n",
    "2018年6月18日\n",
    " DOI\n",
    "https://doi.org/10.1007/978-3-319-42913-7_56-1\n",
    "出版商名称\n",
    "Springer，Cham\n",
    " 在线ISBN\n",
    "978-3-319-42913-7\n",
    "电子书包\n",
    "物理和天文学\n",
    "重印和权限\n",
    "操作\n",
    "超过1000万份科学文档触手可及\n",
    "\n",
    "\n",
    "Academic Edition\n",
    "企业版\n",
    "家 IMPRESSUM 法律信息 隐私声明 我们如何使用cookies 无障碍 联系我们\n",
    "施普林格自然\n",
    "©2018 Springer Nature Switzerland AG。部分斯普林格性质。\n",
    "\n",
    "未登入 东吴大学图书馆（3000131270） - 5102 SpringerLink China OAC National Consortium（3000202650） - 7217 SpringerLink江苏电子书联盟2010-2012版权年（3000203264） - 10786 SLCC江苏（3000803042） - LID 4548 - SpringerLink江苏eBo联盟2006- 9版权所有（3001144869） - SpringerLink江苏电子书联盟2016-2018版权所有年份（3002351942） - 13433 SpringerLink江苏电子书联盟2013-2015版权所有（3991438083） - SLCC江苏电子期刊联盟2015-2017（3991465546） 42.244.24.216\n",
    "\n",
    "沪ICP备15051854号-2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
